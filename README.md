

# ğŸŒŸ **Micrograd**

### *A Tiny Autograd Engine That Teaches You How Deep Learning REALLY Works*

âœ¨ *Clean â€¢ Elegant â€¢ Educational â€¢ Minimal â€¢ Powerful*

---

<div align="center">



**A heartfelt tribute to Karpathyâ€™s tiny autograd engine.
Rebuilt. Explained. Beautified.**

</div>

---

# ğŸ“˜ **Table of Contents**

1. ğŸŒ± Introduction
2. âš¡ Why Micrograd Exists
3. ğŸ§  How Micrograd Works
4. ğŸ§© Building Blocks (Value Class Explained)
5. ğŸ”™ Backpropagation â€“ Simple Explanation
6. ğŸ—ï¸ Building & Training a Small MLP
7. ğŸ”„ Fan-Out, Accumulation & Advanced Backprop Concepts
8. ğŸ†š Micrograd vs PyTorch
9. ğŸ“Š Demo Code
10. ğŸ Final Summary

---

# ğŸŒ± **1. Introduction**

**Micrograd** is a *tiny automatic differentiation engine* built by **Andrej Karpathy**.
It is only **~100 lines of code**, yet it teaches:

* âœ” what is a computation graph
* âœ” how forward pass builds the graph
* âœ” how backward pass walks through it
* âœ” how gradients flow
* âœ” how neural nets learnâ€”*from scratch*

This repo gives:

ğŸ“Œ **Ultra clean implementation**
ğŸ“Œ **Beginner-friendly commentary**
ğŸ“Œ **MLP training using Micrograd**
ğŸ“Œ **Educational visuals + explanations**

---

# âš¡ **2. Why Micrograd Exists?**

Deep learning libraries like **PyTorch** do:

```python
loss.backward()
```

Magically gradients appear.

But how?

Micrograd shows:

* No magic
* No complex tensors
* No abstractions

Just:

* **a graph**
* **nodes**
* **chain rule**
* **reverse traversal**

This is the *absolute core* of deep learning.

---

# ğŸ§  **3. How Micrograd Works (One Sentence)**

**Micrograd builds a graph of `Value` nodes during the forward pass
and computes gradients by walking backward through that graph.**

Simple. Transparent. Beautiful.

---

# ğŸ§© **4. Value Class â€” The Brain of Micrograd**

```python
class Value:
    def __init__(self, data, _children=(), _op=''):
        self.data = data
        self.grad = 0.0
        self._backward = lambda: None
        self._prev = set(_children)
        self._op = _op
```

### ğŸ” What it does:

* Stores a **number**
* Tracks its **gradient**
* Remembers **which nodes created it**
* Stores **the operation** (+, *, tanhâ€¦)
* Holds a custom **backward function**

This is exactly how PyTorch tensors work â€” but simplified.

---

# ğŸ”™ **5. Backpropagation â€” Simple Explanation**

Backprop = â€œHow does changing this input change the final output?â€

### âœ” Step 1: Forward Pass

Builds the graph by performing operations:

```
a â†’ b â†’ c â†’ ... â†’ loss
```

### âœ” Step 2: Set Final Gradient

```
loss.grad = 1
```

### âœ” Step 3: Walk Backward

Use chain rule:

```
parent.grad += child.grad * derivative
```

### âœ” Step 4: Repeat Until All Nodes Updated

This is the heart of deep learning.

---

# ğŸ—ï¸ **6. Building & Training an MLP in Micrograd**

```python
class Neuron:
    def __init__(self, nin):
        self.w = [Value(random.uniform(-1,1)) for _ in range(nin)]
        self.b = Value(0.0)

    def __call__(self, x):
        act = sum((wi*xi for wi,xi in zip(self.w, x)), self.b)
        return act.tanh()
```

Stack neurons â†’ layer
Stack layers â†’ MLP
Forward pass â†’ output
Backward pass â†’ gradients
Update weights â†’ learning

This is literally how PyTorch works internally.

---

# ğŸ”„ **7. Advanced Concepts (Made Easy)**

### ğŸ”¹ **Fan-Out**

When a value is used multiple times, its gradient appears multiple times.

### ğŸ”¹ **Gradient Accumulation**

```
v.grad += incoming_grad
```

NOT replace â€” **add**.

### ğŸ”¹ **New Operations**

Micrograd easily extends to:

* tanh
* exp
* power
* relu
* sigmoid

Just define the forward + backward rule.

---

# ğŸ†š **8. Micrograd vs PyTorch**

| Feature                     | Micrograd           | PyTorch                  |
| --------------------------- | ------------------- | ------------------------ |
| Purpose                     | Teaching            | Production Deep Learning |
| Speed                       | Slow                | Extremely Fast (GPU/TPU) |
| Supports Tensors?           | âŒ No, only scalars  | âœ” Yes                    |
| Builds Graph Automatically? | âœ” Yes               | âœ” Yes                    |
| Backprop?                   | âœ” Manual chain rule | âœ” Highly optimized       |
| Best Use                    | Learning internals  | Real-world models        |

---

# ğŸ“Š **9. Full Working Demo Code**

```python
from micrograd.engine import Value

# tiny dataset
xs = [
    [Value(2.0), Value(3.0)],
    [Value(1.0), Value(-1.0)],
]

ys = [Value(1.0), Value(-1.0)]

# simple neuron
n = Neuron(2)

for epoch in range(20):
    ypred = [n(x) for x in xs]
    loss = sum((yout - yt)**2 for yout, yt in zip(ypred, ys))

    # backward
    for p in n.parameters(): p.grad = 0
    loss.backward()

    # update
    for p in n.parameters():
        p.data -= 0.1 * p.grad

    print(epoch, loss.data)
```

---

# ğŸ **10. Final Summary**

Micrograd teaches you:

âœ” how neural nets work
âœ” how gradients flow
âœ” how autograd engines function
âœ” how forward & backward graph traversal works
âœ” how to build models **from scratch**

It is the **cleanest**,
**purest**,
**most elegant**
deep learning educational tool ever created.

---

### â¤ï¸ If this helped, â­ the repo!

