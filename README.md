# ğŸŒŸ MICROGRAD  


---

# ğŸš€ What is Micrograd? 

Micrograd is a **tiny automatic differentiation engine** created by Andrej Karpathy.  
It helps you compute gradients for any mathematical expression by:

1. **Building a computation graph during the forward pass**
2. **Flowing gradients backward through the graph using chain rule**
3. **Accumulating gradients on every node**
4. **Allowing you to optimize neural network parameters**

Think of Micrograd as:

ğŸ§® A calculator that not only computes your answerâ€¦  
â€¦but also tells you **how the answer changes if every input is nudged slightly** â€” automatically.

Or in simpler terms:

> â€œMicrograd lets you write a math expression normallyâ€¦  
> and magically gives you all the derivatives needed for training neural networks.â€

---

# ğŸŒ±Why Micrograd Exists

Modern deep learning frameworks like **PyTorch** and **TensorFlow** have powerful autograd engines.  
Micrograd is a **minimal** version of that engine.

- No GPU  
- No tensors  
- No layers  
- No optimizers  
- Just **values** and **gradients**.

It teaches you:

âœ” how computation graphs are built  
âœ” how gradients flow backward  
âœ” how chain rule works in real code  
âœ” how neural networks learn under the hood  

---

# ğŸ§  How Micrograd Works (In One Sentence)
> Micrograd stores every operation between numbers as a node in a graph,  
> then applies the **chain rule** from the output backward to compute gradients.

---

# ğŸ“˜ This Covers

1. Simple derivative (f(x)=xÂ²)  
2. Numerical derivative check  
3. Multiple derivatives (f(x,y)=xÂ·y)  
4. Full manual backpropagation example (your entire step-by-step story)  
5. Microgradâ€™s backward() system  
6. Gradient accumulation  
7. Difference between PyTorch & Micrograd  
8. Full annotated Micrograd code  

---

This  is built for **students**, **beginners**, **developers**, and **anyone trying to understand autograd**.

# ğŸ“Œ 1. Single Derivative â€” f(x) = xÂ²

Letâ€™s start with the simplest function:

\[
f(x) = x^2
\]

The derivative is:

\[
f'(x) = 2x
\]

Now let's evaluate both at \( x = 3 \):

```
f(3)  = 3Â² = 9  
df(3) = 2Ã—3 = 6
```

So:

- The function value is **9**
- The slope at that point is **6**

This means:

> â€œIf you nudge x a tiny bit, the output changes 6Ã— that tiny amount.â€

That's what gradient means.

---

# ğŸ“Œ 2. Numerical Derivative (Finite Difference Method)

Now we verify the derivative numerically.

We use a very tiny number **Îµ (epsilon)** and compute:

\[
\frac{f(x+\varepsilon) - f(x)}{\varepsilon}
\]

This should be close to the real derivative.

---

### âœ” Python-style demonstration

```python
def f(x):
    return x*x

x = 3.0
eps = 1e-6

# analytical derivative
df_analytic = 2*x

# numerical derivative
df_numerical = (f(x+eps) - f(x)) / eps

print("f(3) =", f(x))
print("Analytical df =", df_analytic)
print("Numerical df =", df_numerical)
```

### âœ” Output (Example)

```
f(3) = 9
Analytical df = 6
Numerical df â‰ˆ 5.99999999976
```

---

# ğŸ“Œ 3. Interpretation

Point to the output and say:

> â€œSee this? The **analytical derivative (6)** and the **numerical derivative (â‰ˆ6)** match.  
> Micrograd automates this for *every node* in a computation graph.â€

This is the core idea behind **automatic differentiation**.

---

# ğŸ“Œ 4. Why Numerical Derivative First?

Before understanding backpropagation, we must understand:

- A function
- Its slope
- How to approximate slope numerically
- How analytic slope and numerical slope match

Micrograd does NOT use numerical derivatives â€” that would be extremely slow.

It uses **symbolic chain rule** across a graph.  
But the *idea* is exactly the same as what we just computed.

---

# ğŸ“Œ 5. Summary

| Concept | Meaning |
|--------|---------|
| f(x) | Function value |
| fâ€²(x) | Slope at x |
| Numerical derivative | Verify the slope with tiny Îµ |
| Matching values | Shows correctness |
| Micrograd | Automates all of this for giant networks |


---

# ğŸ§  6. How Micrograd Works (One Sentence)

**Micrograd builds a graph of `Value` nodes during the forward pass
and computes gradients by walking backward through that graph.**

Simple. Transparent. Beautiful.

---

# ğŸ§© 7. Value Class â€” The Brain of Micrograd

```python
class Value:
    def __init__(self, data, _children=(), _op=''):
        self.data = data
        self.grad = 0.0
        self._backward = lambda: None
        self._prev = set(_children)
        self._op = _op
```

### ğŸ” What it does:

* Stores a **number**
* Tracks its **gradient**
* Remembers **which nodes created it**
* Stores **the operation** (+, *, tanhâ€¦)
* Holds a custom **backward function**

This is exactly how PyTorch tensors work â€” but simplified.

---

# ğŸ”™ 8. Backpropagation â€” Simple Explanation

Backprop = â€œHow does changing this input change the final output?â€

### âœ” Step 1: Forward Pass

Builds the graph by performing operations:

```
a â†’ b â†’ c â†’ ... â†’ loss
```

### âœ” Step 2: Set Final Gradient

```
loss.grad = 1
```

### âœ” Step 3: Walk Backward

Use chain rule:

```
parent.grad += child.grad * derivative
```

### âœ” Step 4: Repeat Until All Nodes Updated

This is the heart of deep learning.

---

# ğŸ—ï¸ 9. Building & Training an MLP in Micrograd

```python
class Neuron:
    def __init__(self, nin):
        self.w = [Value(random.uniform(-1,1)) for _ in range(nin)]
        self.b = Value(0.0)

    def __call__(self, x):
        act = sum((wi*xi for wi,xi in zip(self.w, x)), self.b)
        return act.tanh()
```

Stack neurons â†’ layer
Stack layers â†’ MLP
Forward pass â†’ output
Backward pass â†’ gradients
Update weights â†’ learning

This is literally how PyTorch works internally.

---

# ğŸ”„ 9. Advanced Concepts (Made Easy)

### ğŸ”¹ Fan-Out

When a value is used multiple times, its gradient appears multiple times.

### ğŸ”¹ **Gradient Accumulation**

```
v.grad += incoming_grad
```

NOT replace â€” **add**.

### ğŸ”¹ **New Operations**

Micrograd easily extends to:

* tanh
* exp
* power
* relu
* sigmoid

Just define the forward + backward rule.

---

# ğŸ†š **10. Micrograd vs PyTorch**

| Feature                     | Micrograd           | PyTorch                  |
| --------------------------- | ------------------- | ------------------------ |
| Purpose                     | Teaching            | Production Deep Learning |
| Speed                       | Slow                | Extremely Fast (GPU/TPU) |
| Supports Tensors?           | âŒ No, only scalars  | âœ” Yes                    |
| Builds Graph Automatically? | âœ” Yes               | âœ” Yes                    |
| Backprop?                   | âœ” Manual chain rule | âœ” Highly optimized       |
| Best Use                    | Learning internals  | Real-world models        |

---

# ğŸ“Š **11. Full Working Demo Code**

```python
from micrograd.engine import Value

# tiny dataset
xs = [
    [Value(2.0), Value(3.0)],
    [Value(1.0), Value(-1.0)],
]

ys = [Value(1.0), Value(-1.0)]

# simple neuron
n = Neuron(2)

for epoch in range(20):
    ypred = [n(x) for x in xs]
    loss = sum((yout - yt)**2 for yout, yt in zip(ypred, ys))

    # backward
    for p in n.parameters(): p.grad = 0
    loss.backward()

    # update
    for p in n.parameters():
        p.data -= 0.1 * p.grad

    print(epoch, loss.data)
```

---

# ğŸ 12. Final Summary

Micrograd teaches you:

âœ” how neural nets work
âœ” how gradients flow
âœ” how autograd engines function
âœ” how forward & backward graph traversal works
âœ” how to build models **from scratch**

It is the **cleanest**,
**purest**,
**most elegant**
deep learning educational tool ever created.

---



